{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data1 = pd.read_csv( 'data/train.csv')\n",
    "\n",
    "data2 = pd.read_csv( 'data/comments.csv')\n",
    "\n",
    "data3 = pd.read_csv( 'data/ar_reviews_100k.csv')\n",
    "\n",
    "data = pd.concat( ( data1, data2, data3)).reset_index(drop=True)\n",
    "\n",
    "import sklearn\n",
    "data = sklearn.utils.shuffle( data)\n",
    "\n",
    "X = data[ 'comment']\n",
    "y = data[ 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Comments in data: 102984\n"
     ]
    }
   ],
   "source": [
    "print( \"Number of Comments in data:\", data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Tokenisation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aminb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop = set( stopwords.words('arabic'))\n",
    "\n",
    "import string\n",
    "punctuation = list( string.punctuation)\n",
    "stop.update( punctuation)\n",
    "\n",
    "import re\n",
    "from clean import wordopt\n",
    "\n",
    "def split_into_words(text):\n",
    "    words = text.split()\n",
    "    return words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    stripped = [re_punc.sub('', w) for w in words]\n",
    "    return stripped\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    stop_words = set(stopwords.words('arabic'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    return words\n",
    "\n",
    "def to_sentence(words):\n",
    "    return ' '.join(words)\n",
    "\n",
    "def denoise_text(text):\n",
    "    text = wordopt( text)\n",
    "    words = split_into_words(text)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the data comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       انا اوصي من هذا المنبر الكل للتوجه الى مراكز ا...\n",
       "1       \\nهناك الكثير لا يفهم المقصود كورونا ليست صعبة...\n",
       "2       الحمد لله ارقام مقبولة مقارنة بدول المنطقة لول...\n",
       "3       انا شخصيا أؤيد ما فرضته السلطات من ضرورة الإدل...\n",
       "4       \\nنفس الشئ في مدينة برشيد مراكز التلقيح مغلقة ...\n",
       "                              ...                        \n",
       "1915    \\nوالله حتى دمرونا وتعداو علينا شي ميقراش\\nشي ...\n",
       "1916                      لقاح موت و سم فيقو ا عباد الله \n",
       "1917    اللقاح غير آمن  ولا يمكن تجاهل عشرات شهادات ال...\n",
       "1918    لنكن واقعيين…لا يجب فرظ التلقيح بالقوة  انا تل...\n",
       "1919    \\n#التنسيقية_الوطنية_للمغاربة_الرافضين_للتلقيح...\n",
       "Name: comment, Length: 1919, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = data.apply( denoise_text)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [اوصي, منبر, توجه, مراكز, تلقيح, صدقوني, امر, ...\n",
       "1       [كثير, يفهم, مقصود, كورونا, صعبة, شباب, لكنها,...\n",
       "2       [حمد, لله, ارقام, مقبولة, مقارنة, بدول, منطقة,...\n",
       "3       [شخصيا, اؤيد, فرضته, سلطات, ضرورة, إدلا, بجواز...\n",
       "4       [مدينة, برشيد, مراكز, تلقيح, مغلقة, غاية, محاس...\n",
       "                              ...                        \n",
       "1915    [دمرونا, وتعداو, علينا, ميقراششي, ميخدمش, فلمي...\n",
       "1916                    [لقاح, موت, سم, فيقو, عباد, الله]\n",
       "1917    [لقاح, آمن, يمكن, تجاهل, عشرات, شهادات, اطبا, ...\n",
       "1918    [لنكن, واقعيين, يجب, فرظ, تلقيح, بالقوة, تلقيت...\n",
       "1919    [تنسيقية, وطنية, مغاربة, رافضين, تلقيح, إجباري...\n",
       "Name: comment, Length: 1919, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(\n",
    "    window=10,\n",
    "    min_count=2,\n",
    "    workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab( vocabulary, progress_per=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38503596, 52006000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.train( vocabulary, total_examples=w2v_model.corpus_count, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save(\"models/word2vec-arabic-moroccen-dialect.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the WordVec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('بلدك', 0.5379806160926819),\n",
       " ('يحمد', 0.5372437238693237),\n",
       " ('بالمال', 0.5169765949249268),\n",
       " ('باكملها', 0.5093464255332947),\n",
       " ('عقباه', 0.45860323309898376),\n",
       " ('يملك', 0.4556187689304352),\n",
       " ('مسؤولي', 0.4080964922904968),\n",
       " ('سائر', 0.405232310295105),\n",
       " ('مغفلين', 0.4018534719944),\n",
       " ('يهتم', 0.3988811671733856)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v_model = Word2Vec.load(\"models/word2vec-arabic.model\")\n",
    "\n",
    "w2v_model.wv.most_similar(\"خبز\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
